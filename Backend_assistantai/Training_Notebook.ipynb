{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf9b00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4813,
     "status": "ok",
     "timestamp": 1745955045029,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "14bf9b00",
    "outputId": "0d37fa0c-3db4-46a6-84e6-8e171c9d93be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
      "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.11/dist-packages (from flair) (1.38.4)\n",
      "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.5.3)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from flair) (6.3.1)\n",
      "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.30.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.4.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.7.0)\n",
      "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.5.10)\n",
      "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.11/dist-packages (from flair) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.9.0.post0)\n",
      "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair) (1.6.1)\n",
      "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.11/dist-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1.0)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
      "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.51.3)\n",
      "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from flair) (0.8.1)\n",
      "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1)\n",
      "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (4.0.0)\n",
      "Requirement already satisfied: intervaltree in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (0.6.2)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.4 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.38.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (0.12.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.13.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (2.0.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.4)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.4->boto3>=1.20.27->flair) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.7)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install flair # install flair using pip\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qj0o0KEBBzzH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qj0o0KEBBzzH",
    "outputId": "6e53cd5c-47ce-451c-d115-1a2fc3e54c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.5\n",
      "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "!pip install --upgrade numpy  # Replace with desired version\n",
    "!pip install vaderSentiment # install flair using pip\n",
    "from flair.models import TextClassifier # import flair after installing it.\n",
    "from flair.data import Sentence\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ab0fe",
   "metadata": {
    "executionInfo": {
     "elapsed": 5388,
     "status": "aborted",
     "timestamp": 1745955181986,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "bf5ab0fe"
   },
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!', '-']\n",
    "data_file = open('/content/drive/MyDrive/Colab Notebooks/wrong_answers_record.json').read()\n",
    "intents = json.loads(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64579aba",
   "metadata": {
    "executionInfo": {
     "elapsed": 12067,
     "status": "aborted",
     "timestamp": 1745946071838,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "64579aba"
   },
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        # take each word and tokenize it\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        # adding documents\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # adding classes to our class list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c19ab",
   "metadata": {
    "executionInfo": {
     "elapsed": 12091,
     "status": "aborted",
     "timestamp": 1745946071865,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "181c19ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "print (len(classes), \"classes\")\n",
    "\n",
    "print (len(words), \"unique lemmatized words\")\n",
    "\n",
    "\n",
    "pickle.dump(words,open('/path/to/words.pkl','wb'))\n",
    "pickle.dump(classes,open('/path/to/classes.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d1c7d",
   "metadata": {
    "executionInfo": {
     "elapsed": 12088,
     "status": "aborted",
     "timestamp": 1745946071866,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "8b2d1c7d"
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    # initializing bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3559c5",
   "metadata": {
    "executionInfo": {
     "elapsed": 12088,
     "status": "aborted",
     "timestamp": 1745946071868,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "1f3559c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fitting and saving the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=32, verbose=1)\n",
    "model.save('/path/to/model.h5', hist)\n",
    "\n",
    "print(\"model created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07808a",
   "metadata": {
    "executionInfo": {
     "elapsed": 12291,
     "status": "aborted",
     "timestamp": 1745946072071,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "2d07808a"
   },
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "#     print(\"words\", words)\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "#     print(p)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "#     print(res)\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "#     print(results)\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     print(results)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    print(\"ints ---> \",ints)\n",
    "\n",
    "    if ints[0]['intent'] != 'unknown' and float(ints[0]['probability']) < 0.85:\n",
    "        emotion_analysis = get_sentiment_analysis(msg)\n",
    "\n",
    "        if emotion_analysis:\n",
    "            return {\"status\": True}\n",
    "\n",
    "    if float(ints[0]['probability']) < 0.60:\n",
    "        return \"Oops! Couldn't get that, am a new bot in town and learning with time. You can try rephrasing your question.\"\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n",
    "\n",
    "def get_sentiment_analysis(sentence):\n",
    "    flag = False\n",
    "    response = Sentence(sentence.lower())\n",
    "    flair_sentiment.predict(response)\n",
    "    total_sentiment = response.labels\n",
    "\n",
    "    if total_sentiment[0].value == 'NEGATIVE' and total_sentiment[0].score > 0.7:\n",
    "        flag = True\n",
    "\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sid_obj_score = sid_obj.polarity_scores(sentence.lower())\n",
    "    if sid_obj_score['compound'] < 0.3 and sid_obj_score['neu'] != 1.0:\n",
    "        flag = True\n",
    "\n",
    "    return flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271efab",
   "metadata": {
    "executionInfo": {
     "elapsed": 12293,
     "status": "aborted",
     "timestamp": 1745946072074,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "4271efab"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/path/to/model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('/path/to/data.json').read())\n",
    "words = pickle.load(open('/path/to/words.pkl','rb'))\n",
    "classes = pickle.load(open('/path/to/classes.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202e37f",
   "metadata": {
    "executionInfo": {
     "elapsed": 12294,
     "status": "aborted",
     "timestamp": 1745946072077,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "4202e37f"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    msg = input()\n",
    "    res = chatbot_response(msg)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efadb61a",
   "metadata": {
    "executionInfo": {
     "elapsed": 12296,
     "status": "aborted",
     "timestamp": 1745946072079,
     "user": {
      "displayName": "ade",
      "userId": "16803206753357670862"
     },
     "user_tz": -300
    },
    "id": "efadb61a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
